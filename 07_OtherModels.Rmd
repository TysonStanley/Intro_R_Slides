---
title: "Chapter 7: Other Modeling Techniques"
author: "Tyson S. Barrett"
date: "Summer 2017"
institute: "Utah State University"
fontsize: 11pt
output:
  beamer_presentation:
    theme: "metropolis"
    toc: true
    fig_width: 4.5
    fig_height: 3.5
    fig_caption: true
    df_print: default
    keep_tex: true
    incremental: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
dem_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_demographics_11.xpt")
med_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_MedHeath_11.xpt")
men_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_MentHealth_11.xpt")
act_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_PhysActivity_11.xpt")
names(dem_df) <- tolower(names(dem_df))
names(med_df) <- tolower(names(med_df))
names(men_df) <- tolower(names(men_df))
names(act_df) <- tolower(names(act_df))
df <- dem_df %>%
  full_join(med_df, by="seqn") %>%
  full_join(men_df, by="seqn") %>%
  full_join(act_df, by="seqn") %>%
  mutate(marriage = factor(washer(dmdmartl, 77, 99))) %>%
  filter(complete.cases(marriage)) %>%
  mutate(race = factor(ridreth1, 
                       labels=c("MexicanAmerican", "OtherHispanic", 
                                "White", "Black", "Other"))) %>%
  mutate(famsize = as.numeric(washer(dmdfmsiz, 7,9))) %>%
  mutate(dpq010 = washer(dpq010, 7,9),
         dpq020 = washer(dpq020, 7,9),
         dpq030 = washer(dpq030, 7,9),
         dpq040 = washer(dpq040, 7,9),
         dpq050 = washer(dpq050, 7,9),
         dpq060 = washer(dpq060, 7,9),
         dpq070 = washer(dpq070, 7,9),
         dpq080 = washer(dpq080, 7,9),
         dpq090 = washer(dpq090, 7,9),
         dpq100 = washer(dpq100, 7,9)) %>%
  mutate(dep = dpq010 + dpq020 + dpq030 + dpq040 + dpq050 +
               dpq060 + dpq070 + dpq080 + dpq090) %>%
  mutate(dep2 = factor(ifelse(dep >= 10, 1,
                       ifelse(dep < 10, 0, NA))))
df <- df %>%
  mutate(asthma = washer(mcq010, 9, 7),
         asthma = factor(washer(asthma, 2, value = 0),
                         labels = c("No Asthma", "Asthma"))) %>%
  mutate(sed = washer(pad680, 9999, 7777)) %>%
  mutate(cluster = sdmvstra - 89) %>%
  filter(complete.cases(asthma) & complete.cases(dep2)) 
```


# Introduction


## Good Quote
\large

> "Simplicity is the ultimate sophistication."
>
> --- Leonardo da Vinci

## Introduction

We cover, however briefly, modeling techniques that are especially useful to make complex relationships easier to interpret. 

We will focus on:

1. mediation and moderation modeling,
2. methods relating to structural equation modeling (SEM), and 
3. methods applicable to our field from machine learning. 

## Introduction

### An Aside

Although machine learning may appear very different than mediation and SEM, they each have advantages that can help in different situations. 

- For example, SEM is useful when we know there is a high degree of measurement error or our data has multiple indicators for each construct. 
- On the other hand, regularized regression and random forests--two popular forms of machine learning--are great to explore patterns and relationships there are hundreds or thousands of variables that may predict an outcome. 


# Mediation Modeling
## Mediation Modeling

Mediation modeling can be done via several packages. 

SEM framework: 
   + `lavaan` (stands for "latent variable analysis")[^lavaa]. Although it is technically still a "beta" version, it performs very well especially for more simple models. 

Other good ones: 
   + `mediation` 
   + `RMediation`

## Mediation Modeling

We model the following mediation model:
$$
depression = \beta_0 + \beta_1 asthma + \epsilon_1
$$

$$
time_{Sedentary} = \lambda_0 + \lambda_1 asthma + \lambda_2 depression + \epsilon_2
$$

In essence, we believe that asthma increases depression which in turn increases the amount of time spent being sedentary. 

## Mediation Modeling
\footnotesize
```{r, message=FALSE, warning=FALSE,eval=FALSE}
library(lavaan)
df$sed_hr = df$sed/60  ## in hours instead of minutes

## Our model
model1 <- '
  dep ~ asthma
  sed_hr ~ dep + asthma
'
## sem function to run the model
fit <- sem(model1, data = df)
summary(fit)
```

## Mediation Modeling
\tiny
```{r, message=FALSE, warning=FALSE,echo=FALSE}
library(lavaan)
df$sed_hr = df$sed/60  ## in hours instead of minutes

## Our model
model1 <- '
  dep ~ asthma
  sed_hr ~ dep + asthma
'
## sem function to run the model
fit <- sem(model1, data = df)
summary(fit)
```


## Mediation Modeling

From the output we see asthma does predict depression and depression does predict time being sedentary. There is also a direct effect of asthma on sedentary behavior even after controlling for depression. We can further specify the model to have it give us the indirect effect and direct effects tested.

## Mediation Modeling
\footnotesize
```{r, message=FALSE, warning=FALSE,eval=FALSE}
## Our model
model2 <- '
  dep ~ a*asthma
  sed_hr ~ b*dep + c*asthma
 
  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit2 <- sem(model2, data = df)
summary(fit2)
```

## Mediation Modeling
\tiny
```{r, message=FALSE, warning=FALSE,echo=FALSE}
## Our model
model2 <- '
  dep ~ a*asthma
  sed_hr ~ b*dep + c*asthma
 
  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit2 <- sem(model2, data = df)
summary(fit2)
```


## Mediation Modeling

We defined a few things in the model. 

1. We gave the coefficients labels of `a`, `b`, and `c`. 
2. Doing so allows us to define the `indirect` and `total` effects. Here we see the indirect effect, although small, is significant at $p < .001$. The total effect is larger (not surprising) and is also significant.

Also note that we can make the regression equations have other covariates as well if we needed to (i.e. control for age or gender) just as we do in regular regression. 

## Mediation Modeling
\footnotesize
```{r, message=FALSE, warning=FALSE, eval=FALSE}
## Our model
model2.1 <- '
  dep ~ asthma + ridageyr
  sed_hr ~ dep + asthma + ridageyr
'
## sem function to run the model
fit2.1 <- sem(model2.1, data = df)
summary(fit2.1)
```

## Mediation Modeling
\tiny
```{r, message=FALSE, warning=FALSE, echo=FALSE}
## Our model
model2.1 <- '
  dep ~ asthma + ridageyr
  sed_hr ~ dep + asthma + ridageyr
'
## sem function to run the model
fit2.1 <- sem(model2.1, data = df)
summary(fit2.1)
```

## Mediation Modeling

Although we don't show it here, we can also do moderation ("interactions") as part of the mediation model.

This is best done through packages other than `lavaan`.

# Structural Equation Modeling
## Structural Equation Modeling

Instead of summing our depression variable, we can use SEM to run the mediation model from above but use the latent variable of depression instead.
\footnotesize
```{r, message=FALSE, warning=FALSE,eval=FALSE}
## Our model
model3 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
  dep1 ~ a*asthma
  sed_hr ~ b*dep1 + c*asthma

  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit3 <- sem(model3, data = df)
summary(fit3)
```

## Structural Equation Modeling
\tiny

```{r, message=FALSE, warning=FALSE, echo=FALSE}
## Our model
model3 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
  dep1 ~ a*asthma
  sed_hr ~ b*dep1 + c*asthma

  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit3 <- sem(model3, data = df)
summary(fit3)
```

## Structural Equation Modeling

We defined `dep1` as a latent variable using `=~`. 

### Model Fit
Although the model does not fit the data well--"`P-value (Chi-square) = 0.000`"--it is informative for demonstration. We would likely need to find out how the measurement model (`dep1 =~ dpq010 + dpq020 + dpq030 +`) actually fits before throwing it into a mediation model. We can do that via:

## Structural Equation Modeling
\scriptsize
```{r,eval=FALSE}
model4 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
'
fit4 <- cfa(model4, data=df)
summary(fit4)
```

## Structural Equation Modeling
\tiny
```{r,echo=FALSE}
model4 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
'
fit4 <- cfa(model4, data=df)
summary(fit4)
```

## Structural Equation Modeling

Lack of fit in the measurement model. 

- It is possible that these depression questions could be measuring more than one factor. 
- We could explore this using exploratory factor analysis. 
- We don't demonstrate that here, but know that it is possible to do in `R` with a few other packages.

# Machine Learning Techniques
## Machine Learning Techniques

We are briefly going to introduce some machine learning techniques that may be of interest to researchers. We will quickly introduce and demonstrate:

1. Ridge, Lasso and Elastic Net
2. Random Forests

## Ridge, Lasso and Elastic Net

Use the fantastic `glmnet` package. 

- Using the `cv.glmnet()` function we can run the ridge ($alpha = 0$), lasso ($alpha = 1$ which is default), and elastic net ($0 \leq alpha \leq 1$). 
- It turns out that elastic net is the combination of the ridge and lasso methods and the closer `alpha` is to 1 the more it acts like lasso and the closer it is to 0 the more it acts like ridge.

## Ridge, Lasso and Elastic Net
### Lasso and Elastic Net

- variable selection
- large number of predictors
- good prediction

### Ridge

- handles multi-collinearity
- large number of predictors
- good prediction

To learn more see "Introduction to Statistical Learning" by Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie. A free PDF is available on their website.

## glmnet

To use the package, it wants the data in a very specific form. 

1. We need to remove any missingness. We use `na.omit()` to do this. 
2. We take all the predictors (without the outcome) and put it in a data matrix object. We only include a few for the demonstration but you can include *many* predictors. We name ours `X`. 
3. `Y` (a vector) is our outcome.

## Ridge, Lasso and Elastic Net

### Prep the Data
```{r, message=FALSE, warning=FALSE}
df2 <- df %>%
  dplyr::select(riagendr, ridageyr, ridreth3, race, famsize, dep, asthma, sed_hr) %>%
  na.omit
X <- df2 %>%
  dplyr::select(-sed_hr) %>%
  data.matrix
Y <- df2$sed_hr
```

## Ridge, Lasso and Elastic Net

Use the `cv.glmnet()` function to fit the different models. 

- The "cv" refers to cross-validation[^crossval], which we don't discuss here, but it an important topic to become familiar with. Below we fit a ridge, a lasso, and an elastic net model. 
- The elastic net model uses more of the lasso penalty because the `alpha` is closer to 1 than 0.

```{r, message=FALSE, warning=FALSE}
library(glmnet)

fit_ridge <- cv.glmnet(X, Y, alpha = 0)
fit_lasso <- cv.glmnet(X, Y, alpha = 1)
fit_enet  <- cv.glmnet(X, Y, alpha = .8)
```

## Ridge, Lasso and Elastic Net

Selecting an appropriate tuning parameter is best done with plots. 

- These plots show where appropriate `lambda` values are based on the mean squared error of the cross-validated prediction. 
- The vertical dashed lines show a reasonable range of lambda values that can be used.

## Ridge, Lasso and Elastic Net

### For example
```{r, eval=FALSE}
plot(fit_enet)
```

## Ridge, Lasso and Elastic Net

```{r, echo=FALSE}
plot(fit_enet)
```

## Ridge, Lasso and Elastic Net

We can get the coefficients at a reasonable `lambda`. 

- Specifically, we use the "1-SE rule" (near the right hand side vertical dashed lines in the above plots) by `s = "lambda.1se"`. 
- You can directly tell it what `lambda` value you'd like but this is a simple rule of thumb.

```{r, eval=FALSE}
coef(fit_ridge, s = "lambda.1se")
coef(fit_lasso, s = "lambda.1se")
coef(fit_enet, s = "lambda.1se")
```

## Ridge, Lasso and Elastic Net
\tiny
```{r, echo=FALSE}
coef(fit_ridge, s = "lambda.1se")
coef(fit_lasso, s = "lambda.1se")
coef(fit_enet, s = "lambda.1se")
```

## Ridge, Lasso and Elastic Net

Although we briefly introduce these regression methods, they are indeed very important. We highly recommend learning more about them.


## Random Forests

Random forests is another machine learning method that can do fantastic prediction. 

It is built in a very different way than the methods we have discussed up to this point. It is not built on a linear modeling scheme; rather, it is built on classification and regression trees (CART). 

Again, "Introduction to Statistical Learning" is a great resource to learn more.

## Random Forests

Use the `randomForest` package. 

- We specify the model by the formula `sed_hr ~ .` which means we want `sed_hr` to be the outcome and all the rest of the variables to be predictors.

```{r, eval=FALSE}
library(randomForest)

fit_rf <- randomForest(sed_hr ~ ., data = df2)
fit_rf
```

## Random Forests
\tiny
```{r, echo=FALSE}
library(randomForest)

fit_rf <- randomForest(sed_hr ~ ., data = df2)
fit_rf
```



## Random Forests

We can find out which variables were important in the model via:

```{r,eval=FALSE}
par(mfrow=c(1,1))  ## back to one plot per page
varImpPlot(fit_rf)
```

## Random Forests

```{r,echo=FALSE}
par(mfrow=c(1,1))  ## back to one plot per page
varImpPlot(fit_rf)
```

## Random Forests

We can see that age (`ridageyr`) is the most important variable, depression (`dep`) follows, with the family size (`famsize`) the third most important in the random forests model.

# Conclusions
## Conclusions

Although we only discussed these methods briefly, that does not mean they are less important. On the contrary, they are essential upper level statistical methods. This brief introduction hopefully helped you know what `R` is capable of across a wide range of methods.


[^lavaa]: The `lavaan` package has some great vignettes at [http://lavaan.ugent.be/](http://lavaan.ugent.be/) to help with the other types of models it can handle.

[^crossval]: Cross-validation is a common way to reduce over-fitting and make sure your model is generalizable. Generally, you split your data into training and testing sets. We recommend using it as often as you can, especially with these methods but also to make sure your other models are accurate on new data as well.
